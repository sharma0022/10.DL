{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSZZSAY7Jl+CpqhdQb08xm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maheshkumar145/DL_Theory/blob/main/DL_Assignment_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.\tWhat does a SavedModel contain? How do you inspect its content?**\n",
        "\n",
        "**Ans:** SavedModel is a directory containing serialized signatures and the state needed to run them, including variable values and vocabularies.\n",
        "\n",
        "Command Line Interface is used to inspect SavedModel."
      ],
      "metadata": {
        "id": "N-Loxj8XIRc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.\tWhen should you use TF Serving? What are its main features?**\n",
        "\n",
        "**Ans:** TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs."
      ],
      "metadata": {
        "id": "EDECkv1xIQTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.\tHow do you deploy a model across multiple TF Serving instances?**\n"
      ],
      "metadata": {
        "id": "cG2QN8y_IOuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config_list: {\n",
        "\n",
        "  config: {\n",
        "    name: \"model1\",\n",
        "    base_path: \"/tmp/model\",\n",
        "    model_platform: \"tensorflow\"\n",
        "  },\n",
        "  config: {\n",
        "     name: \"model2\",\n",
        "     base_path: \"/tmp/model2\",\n",
        "     model_platform: \"tensorflow\"\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "rKqM-n9q-7I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.\tWhen should you use the gRPC API rather than the REST API to query a model served by TF Serving?**\n",
        "\n",
        "**Ans:** gRPC is roughly 7 times faster than REST when receiving data and roughly 10 times faster than REST when sending data for this specific payload. This is mainly due to the tight packing of the Protocol Buffers."
      ],
      "metadata": {
        "id": "oNi6dvaZINcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.\tWhat are the different ways TFLite reduces a modelâ€™s size to make it run on a mobile or embedded device?**\n",
        "\n",
        "**Ans:** Optimization and Quantization "
      ],
      "metadata": {
        "id": "srmAQYDlIMEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.\tWhat is quantization-aware training, and why would you need it?** \n",
        "\n",
        "**Ans:** Quantization-aware training helps you train DNNs for lower precision INT8 deployment, without compromising on accuracy.\n",
        "\n",
        "It is needed to reducing computation demand and increasing power efficiency"
      ],
      "metadata": {
        "id": "YAcD9ubjIKhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.\tWhat are model parallelism and data parallelism? Why is the latter generally recommended?**\n",
        "\n",
        "**Ans:** Data parallelism is when you use the same model for every thread, but feed it with different parts of the data; model parallelism is when you use the same data for every thread, but split the model among threads.\n",
        "\n",
        "Models using data parallelism increase in their accuracy as training proceeds, but the accuracy starts fluctuating with model parallelism"
      ],
      "metadata": {
        "id": "Ln2OvI6WII9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.\tWhen training a model across multiple servers, what distribution strategies can you use?**\n",
        "\n",
        "**Ans:** Distributed training can be achieved by data-parallelism using synchronous training like Mirrored or Multi Mirrored strategies where the same model is replicated on all the workers."
      ],
      "metadata": {
        "id": "jui-3jcZIHAv"
      }
    }
  ]
}